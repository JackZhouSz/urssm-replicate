{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18a1f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23d2b55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "572f3157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3c977c",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b07fe96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from src.dataset.shape_cor import PairFaustDataset\n",
    "\n",
    "dataset = PairFaustDataset(\n",
    "    phase='train',\n",
    "    data_root='../data/FAUST_r/',\n",
    "    return_evecs='true',\n",
    "    return_faces='true',\n",
    "    num_evecs=200,\n",
    "    return_corr='true',\n",
    "    return_dist='true',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9703c14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tr_reg_000', 'tr_reg_001')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utils.tensor import to_device\n",
    "\n",
    "data = dataset[1]\n",
    "data_x, data_y = to_device(data['first'], device), to_device(data['second'], device)\n",
    "data_x['name'], data_y['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8206341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'verts', 'faces', 'evecs', 'evecs_trans', 'evals', 'mass', 'L', 'dist', 'corr'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e21265ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x['corr'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f9ff493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([4999, 3]) torch.Size([5000])\n",
      "torch.Size([5000, 3]) torch.Size([5000])\n",
      "torch.Size([5000, 3]) torch.Size([5000])\n",
      "torch.Size([5000, 3]) torch.Size([5000])\n",
      "torch.Size([5000, 3]) torch.Size([5000])\n",
      "torch.Size([5000, 3]) torch.Size([5000])\n",
      "torch.Size([5000, 3]) torch.Size([5000])\n",
      "torch.Size([5000, 3]) torch.Size([5000])\n",
      "torch.Size([5000, 3]) torch.Size([5000])\n",
      "torch.Size([5000, 3]) torch.Size([5000])\n",
      "torch.Size([5000, 3]) torch.Size([5000])\n",
      "torch.Size([5000, 3]) torch.Size([5000])\n",
      "torch.Size([5000, 3]) torch.Size([5000])\n",
      "torch.Size([5000, 3]) torch.Size([5000])\n",
      "torch.Size([5000, 3]) torch.Size([5000])\n",
      "torch.Size([5000, 3]) torch.Size([5000])\n",
      "torch.Size([5000, 3]) torch.Size([5000])\n",
      "torch.Size([5000, 3]) torch.Size([5000])\n",
      "torch.Size([5000, 3]) torch.Size([5000])\n",
      "torch.Size([5000, 3]) torch.Size([5000])\n",
      "torch.Size([5000, 3]) torch.Size([5000])\n"
     ]
    }
   ],
   "source": [
    "for id in range(100):\n",
    "    data = dataset[id]\n",
    "    print(data['first']['verts'].shape, data['first']['corr'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee768dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "def pad_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of samples, each sample is a dict like \n",
    "        {'first': {'field1': tensor or str, ...}, 'second': {'field1': tensor or str, ...}}\n",
    "    Returns a dict of dicts, with padded tensors where appropriate and lists elsewhere.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "\n",
    "    # keys at the first level: e.g. 'first', 'second'\n",
    "    for part in batch[0].keys():\n",
    "        out[part] = {}\n",
    "        fields = batch[0][part].keys()\n",
    "\n",
    "        for field in fields:\n",
    "            values = [sample[part][field] for sample in batch]\n",
    "\n",
    "            if field in ['verts', 'faces', 'evecs']:\n",
    "                # Tensor field: pad and stack\n",
    "                lengths = [v.shape[-2] for v in values]\n",
    "                max_len = max(lengths)\n",
    "                padded = []\n",
    "\n",
    "                for v in values:\n",
    "                    pad_amt = (0, 0) + (0, max_len - v.shape[-2])\n",
    "                    padded_v = pad(v, pad_amt, \"constant\", 0)\n",
    "                    padded.append(padded_v)\n",
    "                out[part][field] = torch.stack(padded)\n",
    "                \n",
    "                # Also provide mask for valid (unpadded) elements\n",
    "                # out[part][field + '_mask'] = torch.arange(max_len)[None, :] < torch.tensor(lengths)[:, None]\n",
    "\n",
    "            elif field in ['evals', 'evecs_trans', 'mass', 'corr']:\n",
    "                # Tensor field: pad and stack\n",
    "                lengths = [v.shape[-1] for v in values]\n",
    "                max_len = max(lengths)\n",
    "                padded = []\n",
    "\n",
    "                for v in values:\n",
    "                    pad_amt = (0, max_len - v.shape[-1])\n",
    "                    padded_v = pad(v, pad_amt, \"constant\", 0)\n",
    "                    padded.append(padded_v)\n",
    "                out[part][field] = torch.stack(padded)\n",
    "                \n",
    "                # Also provide mask for valid (unpadded) elements\n",
    "                # out[part][field + '_mask'] = torch.arange(max_len)[None, :] < torch.tensor(lengths)[:, None]\n",
    "\n",
    "            elif field in ['L', 'dist']:\n",
    "                # Tensor field: pad and stack\n",
    "                lengths = [v.shape[-2] for v in values]\n",
    "                max_len = max(lengths)\n",
    "                padded = []\n",
    "\n",
    "                for v in values:\n",
    "                    pad_amt = (0, max_len - v.shape[-2]) + (0, max_len - v.shape[-2])\n",
    "                    padded_v = pad(v, pad_amt, \"constant\", 0)\n",
    "                    padded.append(padded_v)\n",
    "                out[part][field] = torch.stack(padded)\n",
    "                \n",
    "                # Also provide mask for valid (unpadded) elements\n",
    "                # out[part][field + '_mask'] = torch.arange(max_len)[None, :] < torch.tensor(lengths)[:, None]\n",
    "            \n",
    "            else:\n",
    "                # Non-tensor field: collect in list\n",
    "                out[part][field] = values\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aae05ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=8, collate_fn=pad_collate_fn)\n",
    "\n",
    "for batch in loader:\n",
    "    # batch is a dict (if using the collate_fn above)\n",
    "    single_sample = batch  # keys: 'verts', 'mask', etc.\n",
    "    break  # Only take the first sample\n",
    "\n",
    "# batch = to_device(batch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff2a2ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'verts', 'faces', 'evecs', 'evecs_trans', 'evals', 'mass', 'L', 'dist', 'corr'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['first'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d91214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['first']['corr'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bb891b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_shape_batch(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for shape correspondence dataset.\n",
    "    Pads all tensors to the maximum number of vertices in the batch.\n",
    "    \"\"\"\n",
    "    # Find maximum number of vertices in the batch\n",
    "    max_verts_first = max([sample['first']['verts'].shape[0] for sample in batch])\n",
    "    max_verts_second = max([sample['second']['verts'].shape[0] for sample in batch])\n",
    "    \n",
    "    # Find maximum number of faces (if you want to batch faces too)\n",
    "    max_faces_first = max([sample['first']['faces'].shape[0] for sample in batch])\n",
    "    max_faces_second = max([sample['second']['faces'].shape[0] for sample in batch])\n",
    "    \n",
    "    collated_batch = []\n",
    "    \n",
    "    for sample in batch:\n",
    "        collated_sample = {'first': {}, 'second': {}}\n",
    "        \n",
    "        for shape_key in ['first', 'second']:\n",
    "            shape_data = sample[shape_key]\n",
    "            max_verts = max_verts_first if shape_key == 'first' else max_verts_second\n",
    "            max_faces = max_faces_first if shape_key == 'first' else max_faces_second\n",
    "            current_verts = shape_data['verts'].shape[0]\n",
    "            current_faces = shape_data['faces'].shape[0]\n",
    "            \n",
    "            # Copy non-tensor data\n",
    "            collated_sample[shape_key]['name'] = shape_data['name']\n",
    "            \n",
    "            # Pad vertex coordinates (V, 3) -> (max_V, 3)\n",
    "            verts_padded = torch.zeros(max_verts, 3)\n",
    "            verts_padded[:current_verts] = shape_data['verts']\n",
    "            collated_sample[shape_key]['verts'] = verts_padded\n",
    "            \n",
    "            # Pad faces (F, 3) -> (max_F, 3)\n",
    "            # Note: padded faces will have invalid indices, you might want to set them to -1\n",
    "            faces_padded = torch.full((max_faces, 3), -1, dtype=shape_data['faces'].dtype)\n",
    "            faces_padded[:current_faces] = shape_data['faces']\n",
    "            collated_sample[shape_key]['faces'] = faces_padded\n",
    "            \n",
    "            # Pad eigenvectors (V, K) -> (max_V, K)\n",
    "            K = shape_data['evecs'].shape[1]\n",
    "            evecs_padded = torch.zeros(max_verts, K)\n",
    "            evecs_padded[:current_verts] = shape_data['evecs']\n",
    "            collated_sample[shape_key]['evecs'] = evecs_padded\n",
    "            \n",
    "            # Pad transposed eigenvectors (K, V) -> (K, max_V)\n",
    "            evecs_trans_padded = torch.zeros(K, max_verts)\n",
    "            evecs_trans_padded[:, :current_verts] = shape_data['evecs_trans']\n",
    "            collated_sample[shape_key]['evecs_trans'] = evecs_trans_padded\n",
    "            \n",
    "            # Eigenvalues don't need padding (K,)\n",
    "            collated_sample[shape_key]['evals'] = shape_data['evals']\n",
    "            \n",
    "            # Pad Laplacian matrix (V, V) -> (max_V, max_V)\n",
    "            L_padded = torch.zeros(max_verts, max_verts)\n",
    "            L_padded[:current_verts, :current_verts] = shape_data['L']\n",
    "            collated_sample[shape_key]['L'] = L_padded\n",
    "            \n",
    "            # Pad distance matrix (V, V) -> (max_V, max_V)\n",
    "            dist_padded = torch.zeros(max_verts, max_verts)\n",
    "            dist_padded[:current_verts, :current_verts] = shape_data['dist']\n",
    "            collated_sample[shape_key]['dist'] = dist_padded\n",
    "            \n",
    "            # Pad correspondence (V,) -> (max_V,)\n",
    "            corr_padded = torch.full((max_verts,), -1, dtype=shape_data['corr'].dtype)\n",
    "            # print(corr_padded.shape, shape_data['corr'].shape, current_verts)\n",
    "            corr_padded[:len(shape_data['corr'])] = shape_data['corr']\n",
    "            collated_sample[shape_key]['corr'] = corr_padded\n",
    "            \n",
    "            # Create a mask indicating real vs padded vertices\n",
    "            mask = torch.zeros(max_verts, dtype=torch.bool)\n",
    "            mask[:current_verts] = True\n",
    "            collated_sample[shape_key]['mask'] = mask\n",
    "            \n",
    "            # Store original number of vertices and faces for reference\n",
    "            collated_sample[shape_key]['num_verts'] = current_verts\n",
    "            collated_sample[shape_key]['num_faces'] = current_faces\n",
    "        \n",
    "        collated_batch.append(collated_sample)\n",
    "    \n",
    "    # Stack all samples in the batch\n",
    "    batched_data = {'first': {}, 'second': {}}\n",
    "    \n",
    "    for shape_key in ['first', 'second']:\n",
    "        # Stack all tensor fields\n",
    "        for field in ['verts', 'faces', 'evecs', 'evecs_trans', 'evals', 'L', 'dist', 'corr', 'mask']:\n",
    "            batched_data[shape_key][field] = torch.stack([\n",
    "                sample[shape_key][field] for sample in collated_batch\n",
    "            ])\n",
    "        \n",
    "        # Keep non-tensor fields as lists\n",
    "        batched_data[shape_key]['name'] = [sample[shape_key]['name'] for sample in collated_batch]\n",
    "        batched_data[shape_key]['num_verts'] = torch.tensor([sample[shape_key]['num_verts'] for sample in collated_batch])\n",
    "        batched_data[shape_key]['num_faces'] = torch.tensor([sample[shape_key]['num_faces'] for sample in collated_batch])\n",
    "    \n",
    "    return batched_data\n",
    "\n",
    "# Usage with DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=8, \n",
    "    shuffle=True, \n",
    "    # num_workers=2,\n",
    "    collate_fn=collate_shape_batch\n",
    ")\n",
    "\n",
    "# Example of how to use the batched data\n",
    "for batch in dataloader:\n",
    "    # batch['first']['verts'] has shape (batch_size, max_verts, 3)\n",
    "    # batch['first']['mask'] has shape (batch_size, max_verts) - True for real vertices\n",
    "    \n",
    "    first_verts = batch['first']['verts']  # (B, max_V, 3)\n",
    "    first_mask = batch['first']['mask']    # (B, max_V)\n",
    "    \n",
    "    # When processing, you can use the mask to ignore padded vertices\n",
    "    # For example, in loss computation:\n",
    "    # loss = compute_loss(predictions[first_mask], targets[first_mask])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bef7ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_shape_batch_optimized(batch):\n",
    "    \"\"\"\n",
    "    Optimized version using more efficient tensor operations\n",
    "    \"\"\"\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # Pre-calculate all dimensions to avoid repeated calculations\n",
    "    first_verts = [sample['first']['verts'].shape[0] for sample in batch]\n",
    "    second_verts = [sample['second']['verts'].shape[0] for sample in batch]\n",
    "    first_faces = [sample['first']['faces'].shape[0] for sample in batch]\n",
    "    second_faces = [sample['second']['faces'].shape[0] for sample in batch]\n",
    "    \n",
    "    max_verts_first = max(first_verts)\n",
    "    max_verts_second = max(second_verts)\n",
    "    max_faces_first = max(first_faces)\n",
    "    max_faces_second = max(second_faces)\n",
    "    \n",
    "    # Get K (number of eigenvectors) - should be the same for all samples\n",
    "    K = batch[0]['first']['evecs'].shape[1]\n",
    "    \n",
    "    # Get data types from first sample to ensure consistency\n",
    "    verts_dtype = batch[0]['first']['verts'].dtype\n",
    "    faces_dtype = batch[0]['first']['faces'].dtype\n",
    "    evecs_dtype = batch[0]['first']['evecs'].dtype\n",
    "    evals_dtype = batch[0]['first']['evals'].dtype\n",
    "    L_dtype = batch[0]['first']['L'].dtype\n",
    "    dist_dtype = batch[0]['first']['dist'].dtype\n",
    "    corr_dtype = batch[0]['first']['corr'].dtype\n",
    "    \n",
    "    # Initialize output tensors directly with proper shapes and types\n",
    "    batched_data = {\n",
    "        'first': {\n",
    "            'verts': torch.zeros(batch_size, max_verts_first, 3, dtype=verts_dtype),\n",
    "            'faces': torch.full((batch_size, max_faces_first, 3), -1, dtype=faces_dtype),\n",
    "            'evecs': torch.zeros(batch_size, max_verts_first, K, dtype=evecs_dtype),\n",
    "            'evecs_trans': torch.zeros(batch_size, K, max_verts_first, dtype=evecs_dtype),\n",
    "            'evals': torch.zeros(batch_size, K, dtype=evals_dtype),\n",
    "            'L': torch.zeros(batch_size, max_verts_first, max_verts_first, dtype=L_dtype),\n",
    "            'dist': torch.zeros(batch_size, max_verts_first, max_verts_first, dtype=dist_dtype),\n",
    "            'corr': torch.full((batch_size, max_verts_first), -1, dtype=corr_dtype),\n",
    "            'mask': torch.zeros(batch_size, max_verts_first, dtype=torch.bool),\n",
    "            'num_verts': torch.tensor(first_verts, dtype=torch.long),\n",
    "            'num_faces': torch.tensor(first_faces, dtype=torch.long),\n",
    "            'name': []\n",
    "        },\n",
    "        'second': {\n",
    "            'verts': torch.zeros(batch_size, max_verts_second, 3, dtype=verts_dtype),\n",
    "            'faces': torch.full((batch_size, max_faces_second, 3), -1, dtype=faces_dtype),\n",
    "            'evecs': torch.zeros(batch_size, max_verts_second, K, dtype=evecs_dtype),\n",
    "            'evecs_trans': torch.zeros(batch_size, K, max_verts_second, dtype=evecs_dtype),\n",
    "            'evals': torch.zeros(batch_size, K, dtype=evals_dtype),\n",
    "            'L': torch.zeros(batch_size, max_verts_second, max_verts_second, dtype=L_dtype),\n",
    "            'dist': torch.zeros(batch_size, max_verts_second, max_verts_second, dtype=dist_dtype),\n",
    "            'corr': torch.full((batch_size, max_verts_second), -1, dtype=corr_dtype),\n",
    "            'mask': torch.zeros(batch_size, max_verts_second, dtype=torch.bool),\n",
    "            'num_verts': torch.tensor(second_verts, dtype=torch.long),\n",
    "            'num_faces': torch.tensor(second_faces, dtype=torch.long),\n",
    "            'name': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Fill tensors efficiently\n",
    "    for i, sample in enumerate(batch):\n",
    "        # Process 'first' shape\n",
    "        v1 = first_verts[i]\n",
    "        f1 = first_faces[i]\n",
    "        \n",
    "        # Use tensor slicing for efficient copying\n",
    "        batched_data['first']['verts'][i, :v1] = sample['first']['verts']\n",
    "        batched_data['first']['faces'][i, :f1] = sample['first']['faces']\n",
    "        batched_data['first']['evecs'][i, :v1] = sample['first']['evecs']\n",
    "        batched_data['first']['evecs_trans'][i, :, :v1] = sample['first']['evecs_trans']\n",
    "        batched_data['first']['evals'][i] = sample['first']['evals']\n",
    "        batched_data['first']['L'][i, :v1, :v1] = sample['first']['L']\n",
    "        batched_data['first']['dist'][i, :v1, :v1] = sample['first']['dist']\n",
    "        batched_data['first']['corr'][i, :len(sample['first']['corr'])] = sample['first']['corr']\n",
    "        batched_data['first']['mask'][i, :v1] = True\n",
    "        batched_data['first']['name'].append(sample['first']['name'])\n",
    "        \n",
    "        # Process 'second' shape\n",
    "        v2 = second_verts[i]\n",
    "        f2 = second_faces[i]\n",
    "        \n",
    "        batched_data['second']['verts'][i, :v2] = sample['second']['verts']\n",
    "        batched_data['second']['faces'][i, :f2] = sample['second']['faces']\n",
    "        batched_data['second']['evecs'][i, :v2] = sample['second']['evecs']\n",
    "        batched_data['second']['evecs_trans'][i, :, :v2] = sample['second']['evecs_trans']\n",
    "        batched_data['second']['evals'][i] = sample['second']['evals']\n",
    "        batched_data['second']['L'][i, :v2, :v2] = sample['second']['L']\n",
    "        batched_data['second']['dist'][i, :v2, :v2] = sample['second']['dist']\n",
    "        batched_data['second']['corr'][i, :len(sample['second']['corr'])] = sample['second']['corr']\n",
    "        batched_data['second']['mask'][i, :v2] = True\n",
    "        batched_data['second']['name'].append(sample['second']['name'])\n",
    "    \n",
    "    return batched_data\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=8, \n",
    "    shuffle=True, \n",
    "    # num_workers=8,\n",
    "    collate_fn=collate_shape_batch_optimized,\n",
    ")\n",
    "\n",
    "# Example of how to use the batched data\n",
    "for batch in dataloader:\n",
    "    # batch['first']['verts'] has shape (batch_size, max_verts, 3)\n",
    "    # batch['first']['mask'] has shape (batch_size, max_verts) - True for real vertices\n",
    "    \n",
    "    first_verts = batch['first']['verts']  # (B, max_V, 3)\n",
    "    first_mask = batch['first']['mask']    # (B, max_V)\n",
    "    \n",
    "    # When processing, you can use the mask to ignore padded vertices\n",
    "    # For example, in loss computation:\n",
    "    # loss = compute_loss(predictions[first_mask], targets[first_mask])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f98e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_shape_batch_ultra_optimized(batch):\n",
    "    \"\"\"\n",
    "    Ultra-optimized version using vectorized operations where possible\n",
    "    \"\"\"\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # Extract all data first to minimize attribute access\n",
    "    first_data = [sample['first'] for sample in batch]\n",
    "    second_data = [sample['second'] for sample in batch]\n",
    "    \n",
    "    def process_shape_batch(shape_data_list, shape_name):\n",
    "        # Pre-calculate dimensions\n",
    "        verts_counts = [data['verts'].shape[0] for data in shape_data_list]\n",
    "        faces_counts = [data['faces'].shape[0] for data in shape_data_list]\n",
    "        max_verts = max(verts_counts)\n",
    "        max_faces = max(faces_counts)\n",
    "        K = shape_data_list[0]['evecs'].shape[1]\n",
    "        \n",
    "        # Get dtypes from first sample\n",
    "        sample_data = shape_data_list[0]\n",
    "        dtypes = {\n",
    "            'verts': sample_data['verts'].dtype,\n",
    "            'faces': sample_data['faces'].dtype,\n",
    "            'evecs': sample_data['evecs'].dtype,\n",
    "            'evals': sample_data['evals'].dtype,\n",
    "            'L': sample_data['L'].dtype,\n",
    "            'dist': sample_data['dist'].dtype,\n",
    "            'corr': sample_data['corr'].dtype,\n",
    "        }\n",
    "        \n",
    "        # Initialize output tensors\n",
    "        result = {\n",
    "            'verts': torch.zeros(batch_size, max_verts, 3, dtype=dtypes['verts']),\n",
    "            'faces': torch.full((batch_size, max_faces, 3), -1, dtype=dtypes['faces']),\n",
    "            'evecs': torch.zeros(batch_size, max_verts, K, dtype=dtypes['evecs']),\n",
    "            'evecs_trans': torch.zeros(batch_size, K, max_verts, dtype=dtypes['evecs']),\n",
    "            'evals': torch.zeros(batch_size, K, dtype=dtypes['evals']),\n",
    "            'L': torch.zeros(batch_size, max_verts, max_verts, dtype=dtypes['L']),\n",
    "            'dist': torch.zeros(batch_size, max_verts, max_verts, dtype=dtypes['dist']),\n",
    "            'corr': torch.full((batch_size, max_verts), -1, dtype=dtypes['corr']),\n",
    "            'mask': torch.zeros(batch_size, max_verts, dtype=torch.bool),\n",
    "            'num_verts': torch.tensor(verts_counts, dtype=torch.long),\n",
    "            'num_faces': torch.tensor(faces_counts, dtype=torch.long),\n",
    "            'name': [data['name'] for data in shape_data_list]\n",
    "        }\n",
    "        \n",
    "        # Vectorized filling where possible\n",
    "        for i, (data, v_count, f_count) in enumerate(zip(shape_data_list, verts_counts, faces_counts)):\n",
    "            # Use .copy_() for potentially faster copying\n",
    "            result['verts'][i, :v_count].copy_(data['verts'])\n",
    "            result['faces'][i, :f_count].copy_(data['faces'])\n",
    "            result['evecs'][i, :v_count].copy_(data['evecs'])\n",
    "            result['evecs_trans'][i, :, :v_count].copy_(data['evecs_trans'])\n",
    "            result['evals'][i].copy_(data['evals'])\n",
    "            result['L'][i, :v_count, :v_count].copy_(data['L'])\n",
    "            result['dist'][i, :v_count, :v_count].copy_(data['dist'])\n",
    "            result['corr'][i, :len(data['corr'])].copy_(data['corr'])\n",
    "            result['mask'][i, :v_count] = True\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # Process both shapes\n",
    "    first_result = process_shape_batch(first_data, 'first')\n",
    "    second_result = process_shape_batch(second_data, 'second')\n",
    "    \n",
    "    return {'first': first_result, 'second': second_result}\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=8, \n",
    "    shuffle=True, \n",
    "    # num_workers=2,\n",
    "    collate_fn=collate_shape_batch_ultra_optimized,\n",
    ")\n",
    "\n",
    "# Example of how to use the batched data\n",
    "for batch in dataloader:\n",
    "    # batch['first']['verts'] has shape (batch_size, max_verts, 3)\n",
    "    # batch['first']['mask'] has shape (batch_size, max_verts) - True for real vertices\n",
    "    \n",
    "    first_verts = batch['first']['verts']  # (B, max_V, 3)\n",
    "    first_mask = batch['first']['mask']    # (B, max_V)\n",
    "    \n",
    "    # When processing, you can use the mask to ignore padded vertices\n",
    "    # For example, in loss computation:\n",
    "    # loss = compute_loss(predictions[first_mask], targets[first_mask])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ac1fe",
   "metadata": {},
   "source": [
    "## DiffusionNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd806f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiffusionNet(\n",
       "  (first_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (last_linear): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (blocks): ModuleList(\n",
       "    (0): DiffusionNetBlock(\n",
       "      (diffusion): LearnedTimeDiffusion()\n",
       "      (gradient_features): SpatialGradientFeatures(\n",
       "        (A_re): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (A_im): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (mlp): MiniMLP(\n",
       "        (miniMLP_linear_000): Linear(in_features=384, out_features=128, bias=True)\n",
       "        (miniMLP_activation_000): ReLU()\n",
       "        (miniMLP_dropout_001): Dropout(p=0.5, inplace=False)\n",
       "        (miniMLP_linear_001): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (miniMLP_activation_001): ReLU()\n",
       "        (miniMLP_dropout_002): Dropout(p=0.5, inplace=False)\n",
       "        (miniMLP_linear_002): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): DiffusionNetBlock(\n",
       "      (diffusion): LearnedTimeDiffusion()\n",
       "      (gradient_features): SpatialGradientFeatures(\n",
       "        (A_re): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (A_im): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (mlp): MiniMLP(\n",
       "        (miniMLP_linear_000): Linear(in_features=384, out_features=128, bias=True)\n",
       "        (miniMLP_activation_000): ReLU()\n",
       "        (miniMLP_dropout_001): Dropout(p=0.5, inplace=False)\n",
       "        (miniMLP_linear_001): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (miniMLP_activation_001): ReLU()\n",
       "        (miniMLP_dropout_002): Dropout(p=0.5, inplace=False)\n",
       "        (miniMLP_linear_002): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): DiffusionNetBlock(\n",
       "      (diffusion): LearnedTimeDiffusion()\n",
       "      (gradient_features): SpatialGradientFeatures(\n",
       "        (A_re): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (A_im): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (mlp): MiniMLP(\n",
       "        (miniMLP_linear_000): Linear(in_features=384, out_features=128, bias=True)\n",
       "        (miniMLP_activation_000): ReLU()\n",
       "        (miniMLP_dropout_001): Dropout(p=0.5, inplace=False)\n",
       "        (miniMLP_linear_001): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (miniMLP_activation_001): ReLU()\n",
       "        (miniMLP_dropout_002): Dropout(p=0.5, inplace=False)\n",
       "        (miniMLP_linear_002): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3): DiffusionNetBlock(\n",
       "      (diffusion): LearnedTimeDiffusion()\n",
       "      (gradient_features): SpatialGradientFeatures(\n",
       "        (A_re): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (A_im): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (mlp): MiniMLP(\n",
       "        (miniMLP_linear_000): Linear(in_features=384, out_features=128, bias=True)\n",
       "        (miniMLP_activation_000): ReLU()\n",
       "        (miniMLP_dropout_001): Dropout(p=0.5, inplace=False)\n",
       "        (miniMLP_linear_001): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (miniMLP_activation_001): ReLU()\n",
       "        (miniMLP_dropout_002): Dropout(p=0.5, inplace=False)\n",
       "        (miniMLP_linear_002): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.module.diffusionnet import DiffusionNet\n",
    "\n",
    "feature_extractor = DiffusionNet(\n",
    "    in_channels=128,\n",
    "    out_channels=256,\n",
    "    cache_dir='../data/FAUST_r/diffusion',\n",
    "    input_type='wks',\n",
    ").to(device)\n",
    "feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71ce46d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_x_batch = feature_extractor(\n",
    "    batch['first']['verts'],\n",
    "    batch['first']['faces'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e727698",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_x = feature_extractor(\n",
    "    data_x['verts'].unsqueeze(0),\n",
    "    data_x['faces'].unsqueeze(0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e7a4164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4999, 3]),\n",
       " torch.Size([5000, 3]),\n",
       " torch.Size([9994, 3]),\n",
       " torch.Size([9996, 3]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "verts = data_x['verts']  # e.g. [4999, 3]\n",
    "faces = data_x['faces']  # e.g. [9994, 3]\n",
    "verts_pad = F.pad(verts, (0, 0, 0, 1), \"constant\", 0) # e.g. [5000, 3]\n",
    "faces_pad = F.pad(faces, (0, 0, 0, 2), \"constant\", 0) # e.g. [9996, 3]\n",
    "verts.shape, verts_pad.shape, faces.shape, faces_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e6e8233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verts_pad[-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fcda27a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4999, 256])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_x = feature_extractor(\n",
    "    verts.unsqueeze(0),\n",
    "    faces.unsqueeze(0),\n",
    ")\n",
    "feat_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "157cf94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5000, 256])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_x_pad = feature_extractor(\n",
    "    verts_pad.unsqueeze(0),\n",
    "    faces_pad.unsqueeze(0),\n",
    ")\n",
    "feat_x_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9793097d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0007, device='cuda:0', grad_fn=<NormBackward1>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(feat_x - feat_x_pad[0, :-1, :]).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab3b0439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5000, 3]), torch.Size([2, 9996, 3]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verts_batch = torch.stack([verts_pad, data_y['verts']], dim=0)\n",
    "faces_batch = torch.stack([faces_pad, data_y['faces']], dim=0)\n",
    "verts_batch.shape, faces_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a05d7c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5000, 256])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_x_batch = feature_extractor(\n",
    "    verts_batch,\n",
    "    faces_batch,\n",
    ")\n",
    "feat_x_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cc310905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0012, device='cuda:0', grad_fn=<NormBackward1>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(feat_x - feat_x_batch[0, :-1, :]).norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4cfd7a",
   "metadata": {},
   "source": [
    "### Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57bf11b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4999, 3])\n",
      "size: 1 time: 0.1654 throughput: 60.4560 sample/second\n",
      "torch.Size([2, 4999, 3])\n",
      "size: 2 time: 0.3456 throughput: 57.8706 sample/second\n",
      "torch.Size([3, 4999, 3])\n",
      "size: 3 time: 0.4803 throughput: 62.4624 sample/second\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4999, 3])\n",
      "size: 4 time: 0.6472 throughput: 61.8069 sample/second\n",
      "torch.Size([5, 4999, 3])\n",
      "size: 5 time: 0.7743 throughput: 64.5778 sample/second\n",
      "torch.Size([6, 4999, 3])\n",
      "size: 6 time: 0.9187 throughput: 65.3091 sample/second\n",
      "torch.Size([7, 4999, 3])\n",
      "size: 7 time: 1.1300 throughput: 61.9484 sample/second\n",
      "torch.Size([8, 4999, 3])\n",
      "size: 8 time: 1.2073 throughput: 66.2641 sample/second\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for size in range(1, 9):\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=size,\n",
    "        collate_fn=pad_collate_fn,\n",
    "    )\n",
    "\n",
    "    for batch, data in enumerate(loader):\n",
    "        data = to_device(data, device)\n",
    "        print(data['first']['verts'].shape)\n",
    "        start_time = time.time()\n",
    "        for _ in range(10):\n",
    "            feat_x_batch = feature_extractor(\n",
    "                data['first']['verts'],\n",
    "                data['first']['faces'],\n",
    "            )\n",
    "        end_time = time.time()\n",
    "        print(f'size: {size} time: {(end_time - start_time):.4f} throughput: {(size * 10 / (end_time - start_time)):.4f} sample/second')\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff038b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 9 throughput: 65.7034 sample/second\n",
      "size: 10 throughput: 65.8434 sample/second\n",
      "size: 11 throughput: 65.4665 sample/second\n",
      "size: 12 throughput: 64.6875 sample/second\n",
      "size: 13 throughput: 66.5755 sample/second\n",
      "size: 14 throughput: 61.2642 sample/second\n",
      "size: 15 throughput: 67.1292 sample/second\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for size in range(9, 16):\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=size,\n",
    "        collate_fn=pad_collate_fn,\n",
    "    )\n",
    "\n",
    "    for batch, data in enumerate(loader):\n",
    "        data = to_device(data, device)\n",
    "        print(data['first']['verts'].shape)\n",
    "        start_time = time.time()\n",
    "        for _ in range(10):\n",
    "            feat_x_batch = feature_extractor(\n",
    "                data['first']['verts'],\n",
    "                data['first']['faces'],\n",
    "            )\n",
    "        end_time = time.time()\n",
    "        print(f'size: {size} throughput: {(size * 10 / (end_time - start_time)):.4f} sample/second')\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd93d52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dongliang-cao-2023-cvpr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
